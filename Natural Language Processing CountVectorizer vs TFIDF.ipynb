{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer vs TFIDF\n",
    "\n",
    "description.__doc__ <br>\n",
    "\"\"\" A simple way to see count vectorizer and term frequency inverse document frequency.\n",
    "<br>\n",
    "Args:\n",
    "  CountVectorizer, TFIDF, Pandas and Random Text\n",
    "<br>  \n",
    "Returns:\n",
    "  Easy overview of the difference of CountVectorizer and TFIDF<br>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[\"Five little monkeys sitting in a tree\",\"One monkey fell down from the tree and hurt her knee\",\"Why me? Oh the humanity!\"],[\"Six large monkeys eating in a tree\",\"Two monkey fell down and hurt their knees\",\"Why me? Oh the humanity!\"]]\n",
    "text2 = \"This string is already flatter than the earth!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Five little monkeys sitting in a tree', 'One monkey fell down from the tree and hurt her knee', 'Why me? Oh the humanity!', 'Six large monkeys eating in a tree', 'Two monkey fell down and hurt their knees', 'Why me? Oh the humanity!']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "flat_text = itertools.chain.from_iterable(text)\n",
    "print(list(flat_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crude way of OOP for NLP\n",
    "class WordProcess:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.word_count = defaultdict(int) # dict with default value 0 for new keys\n",
    "        \n",
    "    def tokenize_list_of_list(self):#Brute tokenizer\n",
    "        flat_text = list(itertools.chain.from_iterable(self.text))\n",
    "        flat_text = \" \".join(flat_text)\n",
    "        for word in flat_text.split(\" \"):\n",
    "          self.word_count[word] += 1\n",
    "             \n",
    "my_text = WordProcess(text)    \n",
    "my_text.tokenize_list_of_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Five': 1, 'little': 1, 'monkeys': 2, 'sitting': 1, 'in': 2, 'a': 2, 'tree': 3, 'One': 1, 'monkey': 2, 'fell': 2, 'down': 2, 'from': 1, 'the': 3, 'and': 2, 'hurt': 2, 'her': 1, 'knee': 1, 'Why': 2, 'me?': 2, 'Oh': 2, 'humanity!': 2, 'Six': 1, 'large': 1, 'eating': 1, 'Two': 1, 'their': 1, 'knees': 1})\n"
     ]
    }
   ],
   "source": [
    "print(my_text.word_count) # Basically we are getting the counts for each word. That is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Five little monkeys sitting in a tree One monkey fell down from the tree and hurt her knee Why me? Oh the humanity!', 'Five little monkeys sitting in a tree One monkey fell down and hurt her knee Why me? Oh the humanity!']\n",
      "['text1', 'text2']\n"
     ]
    }
   ],
   "source": [
    "# Crude Pandas Preprocessing\n",
    "text_names = []\n",
    "texts = []\n",
    "for i in range(0,len(text)):\n",
    "  text_names.append(\"text\" + str(i+1))\n",
    "  texts.append(\" \".join(text[i]))\n",
    "print(lst)\n",
    "print(text_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Document                                               Text\n",
      "0    text1  Five little monkeys sitting in a tree One monk...\n",
      "1    text2  Six large monkeys eating in a tree Two monkey ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(text_names, texts), columns=['Document','Text'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference on preprocessing for NLP\n",
    "The main difference is that a better processor, might be able to do lemmitization and stemming of words. \n",
    "Stemming would bring certain words in our texts down. Like the stem of fell is fall and eating would be eat.\n",
    "We would end up with fewer words in total. And that computationally is better. Modelwise it might not be.\n",
    "The below CountVectorizer does not do this and is almost as crude as the WordProcess class above for this particular text. However it is way better on longer texts. The point is though, a lot of data will be generated. The data is high in cardinallity up to a point, as it approaches some limit in language it will overlap pretty well with the real language in question. We can see the law of large numbers similarly for langugage except, it may require a lot less to get most of vocabulary out than would be expected from a normal distribution. \n",
    "The problem though for NLP is that MEANING isn't extracted at all. It has to be infered, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eating', 'fell', 'humanity', 'hurt', 'knee', 'knees', 'large', 'little', 'monkey', 'monkeys', 'oh', 'sitting', 'tree']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "count_train = count_vectorizer.fit_transform(df[\"Text\"])\n",
    "print(count_vectorizer.get_feature_names()) # The Columns if you will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train.A  # The counts of each word in a vector for each document. Making the CountVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 0.25 0.25 0.25 0.35 0.00 0.00 0.35 0.25 0.25 0.25 0.35 0.50]\n",
      " [0.39 0.28 0.28 0.28 0.00 0.39 0.39 0.00 0.28 0.28 0.28 0.00 0.28]]\n"
     ]
    }
   ],
   "source": [
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(df[\"Text\"])\n",
    "print(f\"{tfidf_train.A}\") # TFIDF vector weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-idf formula \n",
    "\n",
    "The idea is to find words that are very important by frequency in certain documents but not across all documents. If say Zebra is mentioned 5 times in one text and in no other it should be relevant. \n",
    "To do this the frequencies counts/total words are used and multiplied with log(N / d). The log of the number of documents to number of documents with the term ratio. In practise when the algorithem runs it will smoothing new words by adding 1 to the denominator when the count in the vocabulary is 0.  \n",
    "So returned is weighted vectors. Like space-time, or a circle on a sphere or any number of things that are adjusted and weighted in some manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chance for TF-idf formular to be something different - non-linear and ReLU\n",
    "The first time I examined the formula I really liked the log term. As log(1) = 0 would remove the word completly from being used. This is a familiar pattern I see in stronger models. The ability to ignore things.\n",
    "The first part of the equation, that is a count of zero will set the weight to zero. But so will close to presence in all documents.\n",
    "To me this looks a lot like rectified linear unit ReLU in neural networks. And also the SVM support vectors. And other machine learning regularization strategies. That is we can ignore large parts of our data, which is similar to life. Pick out the reducible parts we can do something with, if possible, to understand the phenomenon.\n",
    "\n",
    "In practise however, the TfidfVectorizer can instanciated with a min_df, max_df range that would remove words that are not common enough or too common. It can also be limited by max_features setting a limit to how much of the data is to be used. \n",
    "\n",
    "These are all trying to accomplish the same thing. Reduction. \n",
    "\n",
    "Some ideas on this field:\n",
    "- one: do bootstrap vectors with the corpus, and compare to our corpus #Answers is it more meaningfull than random\n",
    "- two: split up the data with cross validation rather than train test split\n",
    "- three: funnel several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer(stop_words=\"english\", max_df=0.7) Clipping off Non-Linearities\n",
    "\"\"\"\n",
    ":param: max_df\n",
    "float in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
